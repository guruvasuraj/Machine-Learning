{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of a Recurrent Convolutional Network. We tried to simualate a model which is like the one mentioned in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "from scipy.special import comb\n",
    "import itertools\n",
    "from collections import Counter \n",
    "import theano\n",
    "theano.config.mode = 'FAST_COMPILE' #setting the mode of theano to be Fast_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements for keras.\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "'''\n",
    "Get the file in which the data is present and read the lines in the file.\n",
    "'''\n",
    "#######################################################################\n",
    "def readFile(fileName):\n",
    "    with open(fileName,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the labelled texts of the imdb data set to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_read = readFile('imdb_labelled.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and transform data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "This function is used to obtain each sentence in the dataset and strip the unwanted characters that will not help much with the\n",
    "classification.\n",
    "For eg., we might not be interested in words that contain apostrophes \n",
    "\n",
    "'''\n",
    "###############################################################################################################################\n",
    "def stripnonalphanumeric(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "'''\n",
    "This function is used to split the data based on the available classes. This function takes the text along with its\n",
    "labels as the input since the processed data contains the text and the labels in tab separated format, we split the \n",
    "input into the text as well as labels using tab limitation.\n",
    "\n",
    "Once the data is split, we add them to separate lists for future use.\n",
    "\n",
    "'''\n",
    "#############################################################################################################################\n",
    "def split_data(sent):\n",
    "    neg_sent=[]\n",
    "    pos_sent=[]\n",
    "    for s in sent:\n",
    "        tab_sep_data = s.split('\\t') #split the data\n",
    "        if int(tab_sep_data[1]) == 0:\n",
    "            neg_sent.append(tab_sep_data[0]) #negative sentiment sentences\n",
    "        else:\n",
    "            pos_sent.append(tab_sep_data[0]) #positive sentiment sentences\n",
    "    return pos_sent,neg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent,neg_sent=split_data(lines_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "This function is where the actual data is prepared for further processing. First let's combine differnt data into one, Once\n",
    "combined, we call the stripalphanumeric function to get rid of the charactetrs that do not involve in the classification \n",
    "process. \n",
    "\n",
    "After this process, we split the text so as to get individual words and assign the labels to the sentences\n",
    "\n",
    "'''\n",
    "###############################################################################################################################\n",
    "def prepare_data(pos_sent,neg_sent):\n",
    "    x=pos_sent+neg_sent\n",
    "    x=[stripnonalphanumeric(s) for s in x]\n",
    "    x=[s.split(' ') for s in x]\n",
    "    pos_lab = [[1,0] for p in pos_sent]\n",
    "    neg_lab = [[0,1] for n in neg_sent]\n",
    "    y=np.concatenate([pos_lab,neg_lab],0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=prepare_data(pos_sent,neg_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################################################################\n",
    "'''\n",
    "This function is used to make the sententes in the dataset into similar shape. This is how the function works. Get the max \n",
    "sentence length from the available sentences. If the length of the sentence that is taken into consideration is less than the \n",
    "max size then append the sentence with some 'fill' characteres. In our case we just used '!!FILL!!' to fill the gaps.\n",
    "\n",
    "'''\n",
    "################################################################################################################################\n",
    "def pool_data(x):\n",
    "    append_word = '!!FILL!!' #word to fill the gaps\n",
    "    max_sent_length = max(len(s) for s in x) # maximum length of the sentence in the wholw dataset\n",
    "    created_data = []\n",
    "    for i in range(len(x)):\n",
    "        sent = x[i]\n",
    "        fills = max_sent_length - len(sent) #get the difference\n",
    "        n_sent = sent + [append_word] *fills #applt fills\n",
    "        created_data.append(n_sent)\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pool_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "This function is used to create a dictionary for the available word in the dictionary. It takes all the sentences and runs a\n",
    "counter over the sentences to determine the number of times a particular word appears in the whole dataset(term freuency)\n",
    "\n",
    "Feature extraction is done by considering the most common words. For eg., a word appearing 100 times in a corpus is important\n",
    "than words appearing once or twice in the document.\n",
    "\n",
    "After creating the dictionary of words, we obtain teh indices of the words to create the feature vector.\n",
    "'''\n",
    "################################################################################################################################\n",
    "def vocab_dict_builder(sent):\n",
    "    w_count = Counter(itertools.chain(*sent)) #count the number of occurences of the words\n",
    "    voc_dict = [x[0] for x in w_count.most_common()] #get the most common words\n",
    "    voc_idx_map = {x: i for i, x in enumerate(voc_dict)} #map the word to its index in the order of appearance \n",
    "    return [voc_dict,voc_idx_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,words_idx=vocab_dict_builder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Based on thw above, we try to generate the feature vector. For creating the feature vector, we employ two approaches. The first \n",
    "approach is to binarize the data, which signifies, if a word is pressent or not. \n",
    "The next approach is to obtain the max length of the sentence, append some arbitrary words, obtain its index and jsut mark the \n",
    "presence of the word just by marking its index to the corresponding word.\n",
    "\n",
    "The latter approach is used here.\n",
    "'''\n",
    "################################################################################################################################\n",
    "def feat_vec(data,words,words_idx):\n",
    "    x=np.array([[words_idx[i] for i in d] for d in data])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=feat_vec(data,words,words_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly shuffle data\n",
    "'''\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle]\n",
    "y = y[shuffle].argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "The model that is built here is a simple RCNN model which is in correspondence to the one proposed in the paper. Here we use a \n",
    "CNN-rand varaint of the CNN model. The other CNN models are static and non-static. We took this model, since it is time\n",
    "efficient.\n",
    "\n",
    "Below are the model parameters\n",
    "Reference: http://keras.io/getting-started/sequential-model-guide/\n",
    "'''\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "model = 'CNN-rand'\n",
    "no_features = x.shape[1] #total number of features in each text\n",
    "word_embed = 10 #word embedding size \n",
    "## following the above we will have an input matrix of size 81 X 10  \n",
    "filter_size = [3, 4] #let us take this as the filter sizes\n",
    "num_filters = 50 #number of filters to be applied to the convolution layer\n",
    "## fixing the number of filters since no region size is specified, fixing the number of filters constant\n",
    "dropouts = [0.1, 0.5] #drop out values\n",
    "## drop out values are used at various output producing layers to prevent over-fitting\n",
    "h_units = 11 #number of hidden units\n",
    "iterations = 15 # number of iterations, training process should be repeated.\n",
    "lstm_op=30\n",
    "word_embeddings=None #since the model is cnn-rand no embedding such as Word2Vec has to be done explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Using keras we are going to build a model that uses 1D convolution for the convolution model and Long Short Term Memory(LSTM) as\n",
    "the recurrent neural model. The model that we are going to build has only one convolution  layer and one reccurent layer for \n",
    "simplicity and we do not have any hidden layers in between the two layers.\n",
    "\n",
    "\n",
    "1) We define the model to be Sequential\n",
    "2) Calculate the model weights by using the Embedding funtion which is built within the keras package\n",
    "3) Assign drop out value to the layer.\n",
    "4) Create a 1D Convolutional model with the specified hyper parameters\n",
    "5) Below the convolutional model, we are going to add a layer of LSTM model which serves as the recurrent model\n",
    "6) The output is obtained by applying the activation function\n",
    "\n",
    "\n",
    "Here sigmoid is used as activation function, since the data has only two classes.\n",
    "\n",
    "NOTE: The models are stacked one upon the other and not by having them as separate branches and merging them.\n",
    "'''\n",
    "################################################################################################################################\n",
    "model = Sequential() #selecting the model to be Sequential\n",
    "model.add(Embedding(len(words), word_embed, input_length=no_features))\n",
    "#adding embedding layer to the model\n",
    "model.add(Dropout(dropouts[0])) #dropout for the first layer\n",
    "model.add(Convolution1D(nb_filter=num_filters,filter_length=3,border_mode='valid',activation='relu',subsample_length=1))\n",
    "#constructing a single dimensional convolution layer using rectified linear unit for the activation\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "#Applying a max pooling layer to convert the convolutional features into a single dimensional feature vector\n",
    "model.add(Dropout(dropouts[1]))\n",
    "#drop out for the second layer which is the LSTM\n",
    "model.add(LSTM(lstm_op))\n",
    "#we define a fixed number of outputs from the LSTM, which inturn will be the input to the final output layer.\n",
    "model.add(Dense(1))\n",
    "#defining the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "#output layer activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "This block is used to compile the model that we have built so far. There are various settings for the model. For eg., we have\n",
    "different objectives such as MSE(Mean Suared Error), since the problem that we are dealing with is a 2-class problem, we used\n",
    "binary crossentropy as the objective and we used stochastic gradient descent as the optimizer(other optimizers are rmsprop,\n",
    "adagrad,adam,etc.,) and, with the metrics we just show the accuracy which returns a tuple containing the loss and accuracy.\n",
    "We used the default setting for the SGD optimizer, since it performed well on the data and we do not have to customize the \n",
    "optimizer.\n",
    "'''\n",
    "#################################################################################################################################\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/15\n",
      "487s - loss: 1.2117 - acc: 0.5650 - val_loss: 0.7072 - val_acc: 0.5450\n",
      "Epoch 2/15\n",
      "489s - loss: 0.7054 - acc: 0.4760 - val_loss: 0.6918 - val_acc: 0.5550\n",
      "Epoch 3/15\n",
      "486s - loss: 0.6913 - acc: 0.5150 - val_loss: 0.6702 - val_acc: 0.6100\n",
      "Epoch 4/15\n",
      "506s - loss: 0.5292 - acc: 0.7670 - val_loss: 0.4706 - val_acc: 0.7800\n",
      "Epoch 5/15\n",
      "509s - loss: 0.4132 - acc: 0.7810 - val_loss: 0.6756 - val_acc: 0.7200\n",
      "Epoch 6/15\n",
      "535s - loss: 0.2866 - acc: 0.8830 - val_loss: 0.7837 - val_acc: 0.4550\n",
      "Epoch 7/15\n",
      "470s - loss: 0.5814 - acc: 0.6740 - val_loss: 0.5549 - val_acc: 0.8600\n",
      "Epoch 8/15\n",
      "430s - loss: 0.2189 - acc: 0.9430 - val_loss: 0.0788 - val_acc: 0.9800\n",
      "Epoch 9/15\n",
      "450s - loss: 0.1374 - acc: 0.9570 - val_loss: 0.0706 - val_acc: 0.9900\n",
      "Epoch 10/15\n",
      "444s - loss: 0.2075 - acc: 0.9170 - val_loss: 0.0552 - val_acc: 0.9850\n",
      "Epoch 11/15\n",
      "413s - loss: 0.1655 - acc: 0.9400 - val_loss: 0.0767 - val_acc: 0.9800\n",
      "Epoch 12/15\n",
      "421s - loss: 0.1123 - acc: 0.9650 - val_loss: 0.1313 - val_acc: 0.9400\n",
      "Epoch 13/15\n",
      "439s - loss: 0.1101 - acc: 0.9610 - val_loss: 0.0322 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "429s - loss: 0.1192 - acc: 0.9600 - val_loss: 0.0408 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "436s - loss: 0.1006 - acc: 0.9710 - val_loss: 0.2438 - val_acc: 0.9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xda2e780>"
      ]
     },
     "execution_count": 73,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Finally we fit the data after all the transformations we specify the batch size to be 10 and the numbr of times the data is to be\n",
    "trained to 15. We do not explicitly use a predict or evaluate function, but we set the validation set to be 20% of the actual data\n",
    "so by using the fit function itself, we can get both the training as well as the testing accuracy.\n",
    "'''\n",
    "################################################################################################################################\n",
    "model.fit(x, y, batch_size=10,nb_epoch=iterations,validation_split=0.2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}