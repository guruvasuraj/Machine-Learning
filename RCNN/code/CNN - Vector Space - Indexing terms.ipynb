{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import statements\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split,KFold\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "from scipy.special import comb\n",
    "import itertools\n",
    "from collections import Counter \n",
    "import theano\n",
    "theano.config.mode = 'FAST_COMPILE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Read data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "'''\n",
    "Get the file in which the data is present and read the lines in the file.\n",
    "'''\n",
    "#######################################################################\n",
    "def readFile(fileName):\n",
    "    with open(fileName,'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the labelled texts of the imdb data set to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_read = readFile('imdb_labelled.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and transform data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "This function is used to obtain each sentence in the dataset and strip the unwanted characters that will not help much with the\n",
    "classification.\n",
    "For eg., we might not be interested in words that contain apostrophes \n",
    "\n",
    "'''\n",
    "###############################################################################################################################\n",
    "def stripnonalphanumeric(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "'''\n",
    "This function is used to split the data based on the available classes. This function takes the text along with its\n",
    "labels as the input since the processed data contains the text and the labels in tab separated format, we split the \n",
    "input into the text as well as labels using tab limitation.\n",
    "\n",
    "Once the data is split, we add them to separate lists for future use.\n",
    "\n",
    "'''\n",
    "#############################################################################################################################\n",
    "def split_data(sent):\n",
    "    neg_sent=[]\n",
    "    pos_sent=[]\n",
    "    for s in sent:\n",
    "        tab_sep_data = s.split('\\t') #split the data\n",
    "        if int(tab_sep_data[1]) == 0:\n",
    "            neg_sent.append(tab_sep_data[0]) #negative sentiment sentences\n",
    "        else:\n",
    "            pos_sent.append(tab_sep_data[0]) #positive sentiment sentences\n",
    "    return pos_sent,neg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_sent,neg_sent=split_data(lines_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "This function is where the actual data is prepared for further processing. First let's combine differnt data into one, Once\n",
    "combined, we call the stripalphanumeric function to get rid of the charactetrs that do not involve in the classification \n",
    "process. \n",
    "\n",
    "After this process, we split the text so as to get individual words and assign the labels to the sentences\n",
    "\n",
    "'''\n",
    "###############################################################################################################################\n",
    "def prepare_data(pos_sent,neg_sent):\n",
    "    x=pos_sent+neg_sent\n",
    "    x=[stripnonalphanumeric(s) for s in x]\n",
    "    x=[s.split(' ') for s in x]\n",
    "    pos_lab = [[1,0] for p in pos_sent]\n",
    "    neg_lab = [[0,1] for n in neg_sent]\n",
    "    y=np.concatenate([pos_lab,neg_lab],0)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y=prepare_data(pos_sent,neg_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################################################################\n",
    "'''\n",
    "This function is used to make the sententes in the dataset into similar shape. This is how the function works. Get the max \n",
    "sentence length from the available sentences. If the length of the sentence that is taken into consideration is less than the \n",
    "max size then append the sentence with some 'fill' characteres. In our case we just used '!!FILL!!' to fill the gaps.\n",
    "\n",
    "'''\n",
    "################################################################################################################################\n",
    "\n",
    "def pool_data(x):\n",
    "    append_word = '!!FILL!!'\n",
    "    max_sent_length = max(len(s) for s in x)\n",
    "    created_data = []\n",
    "    for i in range(len(x)):\n",
    "        sent = x[i]\n",
    "        fills = max_sent_length - len(sent)\n",
    "        n_sent = sent + [append_word] *fills\n",
    "        created_data.append(n_sent)\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pool_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "This function is used to create a dictionary for the available word in the dictionary. It takes all the sentences and runs a\n",
    "counter over the sentences to determine the number of times a particular word appears in the whole dataset(term freuency)\n",
    "\n",
    "Feature extraction is done by considering the most common words. For eg., a word appearing 100 times in a corpus is important\n",
    "than words appearing once or twice in the document.\n",
    "\n",
    "After creating the dictionary of words, we obtain teh indices of the words to create the feature vector.\n",
    "'''\n",
    "################################################################################################################################\n",
    "def vocab_dict_builder(sent):\n",
    "    w_count = Counter(itertools.chain(*sent))\n",
    "    voc_dict = [x[0] for x in w_count.most_common()]\n",
    "    voc_idx_map = {x: i for i, x in enumerate(voc_dict)}\n",
    "    return [voc_dict,voc_idx_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words,words_idx=vocab_dict_builder(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Based on thw above, we try to generate the feature vector. For creating the feature vector, we employ two approaches. The first \n",
    "approach is to binarize the data, which signifies, if a word is pressent or not. \n",
    "The next approach is to obtain the max length of the sentence, append some arbitrary words, obtain its index and jsut mark the \n",
    "presence of the word just by marking its index to the corresponding word.\n",
    "\n",
    "The latter approach is used here.\n",
    "'''\n",
    "################################################################################################################################\n",
    "def feat_vec(data,words,words_idx):\n",
    "    x=np.array([[words_idx[i] for i in d] for d in data])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=feat_vec(data,words,words_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly shuffle data\n",
    "'''\n",
    "shuffle = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle]\n",
    "y = y[shuffle].argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "'''\n",
    "The model that is built here is a simple RCNN model which is in correspondence to the one proposed in the paper. Here we use a \n",
    "CNN-rand varaint of the CNN model. The other CNN models are static and non-static. We took this model, since it is time\n",
    "efficient.\n",
    "\n",
    "Below are the model parameters\n",
    "Reference: http://keras.io/getting-started/sequential-model-guide/\n",
    "'''\n",
    "################################################################################################################################\n",
    "\n",
    "model = 'CNN-rand'\n",
    "no_features = x.shape[1] #total number of features in each text\n",
    "word_embed = 10 #word embedding size \n",
    "## following the above we will have an input matrix of size 81 X 10  \n",
    "n_grams= [3, 4] #let us take this as the filter sizes\n",
    "num_filters = 50 #number of filters to be applied to the convolution layer\n",
    "## fixing the number of filters since no region size is specified, fixing the number of filters constant\n",
    "dropouts = [0.1, 0.5] #drop out values\n",
    "## drop out values are used at various output producing layers to prevent over-fitting\n",
    "h_units = 11 #number of hidden units\n",
    "iterations = 15 # number of iterations, training process should be repeated.\n",
    "lstm_op=30\n",
    "word_embeddings=None #since the model is cnn-rand no embedding such as Word2Vec has to be done explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Here we use a graph object as a container to hold different models together. For eg., we use two filters which produces different\n",
    "results. We add a max-pooling layer of length 2 to obtain a single dimensional vector. Flatten is used to combine them into  a \n",
    "single vector.\n",
    "'''\n",
    "################################################################################################################################\n",
    "graph = Graph() #create a graph\n",
    "graph.add_input(name='input', input_shape=(no_features, word_embed)) #add the inputs\n",
    "for gram in n_grams:\n",
    "    #create a one-D convolutional layer with the hyper parameters defined\n",
    "    conv = Convolution1D(nb_filter=num_filters,filter_length=gram,border_mode='valid',activation='relu',subsample_length=1)\n",
    "    #assign a pooling layer\n",
    "    pool = MaxPooling1D(pool_length=2)\n",
    "    #add the convolutional layer and the pooling layer to the graph\n",
    "    graph.add_node(conv, name='C-%s' % gram, input='input')\n",
    "    graph.add_node(pool, name='pool-%s' % gram, input='C-%s' % gram)\n",
    "    #combine the pools\n",
    "    graph.add_node(Flatten(), name='flatten-%s' % gram, input='pool-%s' % gram)\n",
    "    #merge the outputs from different branches and merge them into one\n",
    "if len(n_grams)>1:\n",
    "    graph.add_output(name='output',inputs=['flatten-%s' % gr for gr in n_grams],merge_mode='concat')\n",
    "else:                 \n",
    "    graph.add_output(name='output', input='flatten-%s' % n_grams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Using keras we are going to build a model that uses 1D convolution for the convolution model and Long Short Term Memory(LSTM) as\n",
    "the recurrent neural model. The model that we are going to build has only one convolution  layer and one reccurent layer for \n",
    "simplicity and we do not have any hidden layers in between the two layers.\n",
    "\n",
    "\n",
    "1) We define the model to be Sequential\n",
    "2) Calculate the model weights by using the Embedding funtion which is built within the keras package\n",
    "3) Assign drop out value to the layer.\n",
    "4) Create a 1D Convolutional model with the specified hyper parameters\n",
    "5) Below the convolutional model, we are going to add a layer of LSTM model which serves as the recurrent model\n",
    "6) The output is obtained by applying the activation function\n",
    "\n",
    "\n",
    "Here sigmoid is used as activation function, since the data has only two classes.\n",
    "\n",
    "NOTE: The models are stacked one upon the other and not by having them as separate branches and merging them.\n",
    "'''\n",
    "################################################################################################################################\n",
    "\n",
    "#selecting the model to be Sequential\n",
    "model = Sequential()\n",
    "#adding embedding layer to the model\n",
    "model.add(Embedding(len(words), word_embed, input_length=no_features,weights=word_embeddings))\n",
    "#assign drop out to the convolutional layer\n",
    "model.add(Dropout(dropouts[0], input_shape=(no_features, word_embed)))\n",
    "#add the realization built using graph to the model. Here we are trying to combine various realizations of the model together.\n",
    "#for eg., we used different filter sizes, which provides various results\n",
    "model.add(graph)\n",
    "#adding the hidden units to the model\n",
    "model.add(Dense(h_units))\n",
    "#adda second dropout to the second layer's output\n",
    "model.add(Dropout(dropouts[1]))\n",
    "#using the sigmoid for activation\n",
    "model.add(Activation('sigmoid'))\n",
    "#adding the output layer\n",
    "model.add(Dense(1))\n",
    "#using the sigmoid for activation\n",
    "model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "This block is used to compile the model that we have built so far. There are various settings for the model. For eg., we have\n",
    "different objectives such as MSE(Mean Suared Error), since the problem that we are dealing with is a 2-class problem, we used\n",
    "binary crossentropy as the objective and we used stochastic gradient descent as the optimizer(other optimizers are rmsprop,\n",
    "adagrad,adam,etc.,) and, with the metrics we just show the accuracy which returns a tuple containing the loss and accuracy.\n",
    "We used the default setting for the SGD optimizer, since it performed well on the data and we do not have to customize the \n",
    "optimizer.\n",
    "'''\n",
    "#################################################################################################################################\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 461s - loss: 0.7096 - acc: 0.5160 - val_loss: 0.6943 - val_acc: 0.4900\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 445s - loss: 0.7058 - acc: 0.4930 - val_loss: 0.6936 - val_acc: 0.5050\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 480s - loss: 0.6978 - acc: 0.5070 - val_loss: 0.7074 - val_acc: 0.4950\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 454s - loss: 0.7022 - acc: 0.4880 - val_loss: 0.6924 - val_acc: 0.4950\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 500s - loss: 0.6952 - acc: 0.5010 - val_loss: 0.6915 - val_acc: 0.5050\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 407s - loss: 0.6960 - acc: 0.5060 - val_loss: 0.6897 - val_acc: 0.5100\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 497s - loss: 0.6941 - acc: 0.5010 - val_loss: 0.6845 - val_acc: 0.6350\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 433s - loss: 0.6824 - acc: 0.5790 - val_loss: 0.6697 - val_acc: 0.6150\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 438s - loss: 0.6681 - acc: 0.6240 - val_loss: 0.6394 - val_acc: 0.6750\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 469s - loss: 0.6406 - acc: 0.6810 - val_loss: 0.5928 - val_acc: 0.7600\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 455s - loss: 0.5930 - acc: 0.7360 - val_loss: 0.5227 - val_acc: 0.8200\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 431s - loss: 0.5276 - acc: 0.8060 - val_loss: 0.4357 - val_acc: 0.8650\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 514s - loss: 0.4661 - acc: 0.8500 - val_loss: 0.3529 - val_acc: 0.8900\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 532s - loss: 0.4054 - acc: 0.8920 - val_loss: 0.2761 - val_acc: 0.9350\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 448s - loss: 0.3552 - acc: 0.9240 - val_loss: 0.2215 - val_acc: 0.9450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd9e5518>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################################\n",
    "'''\n",
    "Finally we fit the data after all the transformations we specify the batch size to be 10 and the numbr of times the data is to be\n",
    "trained to 15. We do not explicitly use a predict or evaluate function, but we set the validation set to be 20% of the actual data\n",
    "so by using the fit function itself, we can get both the training as well as the testing accuracy.\n",
    "'''\n",
    "################################################################################################################################\n",
    "model.fit(x, y, batch_size=10,nb_epoch=iterations,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
